{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-28T10:21:24.860453Z",
     "start_time": "2025-06-28T10:21:22.123580Z"
    }
   },
   "source": "!pip install transformers torch sentence-transformers Sastrawi scikit-learn faiss-cpu rank_bm25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.11.0)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (0.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:01.134517Z",
     "start_time": "2025-06-28T10:21:24.995984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ],
   "id": "6862a21d962b69c9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:01.170179Z",
     "start_time": "2025-06-28T10:22:01.146074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 1. LOAD DATA ===\n",
    "file_path = 'data/preprocessed/cleaned_putusan_hasil.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path} not found.\")\n",
    "    exit(1)"
   ],
   "id": "b1e1c1aba79a5dde",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:01.207393Z",
     "start_time": "2025-06-28T10:22:01.194744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 2. PREPROCESSING ===\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stop_words_indonesia = stopword_factory.get_stop_words() + [\"terdakwa\", \"korban\", \"menyatakan\", \"secara\", \"sah\", \"meyakinkan\"]\n",
    "\n",
    "synonyms = {\n",
    "    \"pengeroyokan\": [\"kekerasan bersama-sama\", \"penganiayaan bersama-sama\"],\n",
    "    \"penganiayaan\": [\"kekerasan\", \"penyerangan\"],\n",
    "    \"turut serta\": [\"ikut serta\", \"bersama-sama\"],\n",
    "    \"luka berat\": [\"cedera parah\", \"luka serius\"]\n",
    "}\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = stemmer.stem(text)\n",
    "    for key, syn_list in synonyms.items():\n",
    "        for syn in syn_list:\n",
    "            text = text.replace(syn, key)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words_indonesia])\n",
    "    return text"
   ],
   "id": "826e0f979e8f12d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:05.017928Z",
     "start_time": "2025-06-28T10:22:01.233413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 3. INDOBERT EMBEDDING (PRE-TRAINED) ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "def bert_embed(text: str) -> np.ndarray:\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.max(dim=1).values.squeeze().numpy()\n",
    "        return embeddings / np.linalg.norm(embeddings) if np.linalg.norm(embeddings) != 0 else np.zeros(768)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {e}\")\n",
    "        return np.zeros(768)"
   ],
   "id": "cfa6a873581606b0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:05.063223Z",
     "start_time": "2025-06-28T10:22:05.051117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 4. EKSTRAKSI METADATA ===\n",
    "def extract_metadata(row):\n",
    "    pihak = \"\"\n",
    "    text = f\"{row.get('kata_kunci', '')} {row.get('catatan_amar', '')}\"\n",
    "    names = re.findall(r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b', text)\n",
    "    pihak = \" vs \".join(names[:2]) if len(names) >= 2 else \"Unknown\"\n",
    "    return {\n",
    "        \"case_id\": row.name + 1,\n",
    "        \"no_perkara\": row.get(\"nomor\", \"\"),\n",
    "        \"tanggal\": row.get(\"tanggal_register\", \"\"),\n",
    "        \"pasal\": row.get(\"kata_kunci\", row.get(\"catatan_amar\", \"\")),\n",
    "        \"pihak\": pihak,\n",
    "        \"jenis_perkara\": row.get(\"klasifikasi\", \"\"),\n",
    "        \"hakim_ketua\": row.get(\"hakim_ketua\", \"\"),\n",
    "        \"hakim_anggota\": row.get(\"hakim_anggota\", \"\"),\n",
    "        \"panitera\": row.get(\"panitera\", \"\"),\n",
    "        \"tahun\": row.get(\"tahun\", \"\"),\n",
    "        \"lembaga_peradilan\": row.get(\"lembaga_peradilan\", \"\"),\n",
    "        \"jenis_lembaga_peradilan\": row.get(\"jenis_lembaga_peradilan\", \"\"),\n",
    "        \"tanggal_musyawarah\": row.get(\"tanggal_musyawarah\", \"\"),\n",
    "        \"tanggal_dibacakan\": row.get(\"tanggal_dibacakan\", \"\"),\n",
    "        \"pidana_penjara\": row.get(\"pidana_penjara\", \"\")\n",
    "    }"
   ],
   "id": "3945308ae1df3dec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:05.109541Z",
     "start_time": "2025-06-28T10:22:05.104524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 5. EKSTRAKSI KONTEN KUNCI ===\n",
    "def extract_fakta_dan_putusan(row):\n",
    "    ringkasan_fakta = row.get(\"amar\", \"\")\n",
    "    text = f\"{row.get('amar', '')} {row.get('catatan_amar', '')}\"\n",
    "    barang_bukti = re.findall(r'barang bukti:?\\s*([^\\.\\;]+)', text, re.IGNORECASE)\n",
    "    dakwaan = re.findall(r'dakwaan:?\\s*([^\\.\\;]+)', text, re.IGNORECASE)\n",
    "    ringkasan_fakta = f\"{ringkasan_fakta} Barang Bukti: {barang_bukti[0] if barang_bukti else 'None'}. Dakwaan: {dakwaan[0] if dakwaan else 'None'}.\"\n",
    "    return {\n",
    "        \"ringkasan_fakta\": ringkasan_fakta,\n",
    "        \"putusan_hukum\": row.get(\"catatan_amar\", \"\"),\n",
    "        \"text_full\": f\"{row.get('amar', '')} {row.get('catatan_amar', '')}\"\n",
    "    }"
   ],
   "id": "f797cdff13517094",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:22:49.786613Z",
     "start_time": "2025-06-28T10:22:05.141984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 6. FEATURE ENGINEERING ===\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_indonesia, max_features=15000, sublinear_tf=True, ngram_range=(1, 3))\n",
    "corpus = [preprocess(f\"{row.get('amar', '')} {row.get('catatan_amar', '')}\") for _, row in df.iterrows()]\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "\n",
    "def compute_features(row):\n",
    "    text = preprocess(f\"{row.get('amar', '')} {row.get('catatan_amar', '')}\")\n",
    "    words = re.findall(r'\\b\\w+\\b', text) if text else []\n",
    "    tfidf_vec = tfidf_vectorizer.transform([text]).toarray()[0] if text else np.zeros(15000)\n",
    "    embedding = bert_embed(text)\n",
    "    return {\n",
    "        \"length\": len(words),\n",
    "        \"tfidf\": tfidf_vec.tolist(),\n",
    "        \"indobert_embedding\": embedding.tolist(),\n",
    "        \"qa_pair\": [\n",
    "            {\"question\": \"Apa isi ringkasan fakta?\", \"answer\": row.get(\"amar\", \"\")},\n",
    "            {\"question\": \"Apa putusan hakim?\", \"answer\": row.get(\"catatan_amar\", \"\")},\n",
    "            {\"question\": \"Pasal apa yang diterapkan?\", \"answer\": row.get(\"kata_kunci\", row.get(\"catatan_amar\", \"\"))},\n",
    "            {\"question\": \"Siapa hakim ketua?\", \"answer\": row.get(\"hakim_ketua\", \"\")},\n",
    "            {\"question\": \"Berapa lama pidana penjara?\", \"answer\": row.get(\"pidana_penjara\", \"\")}\n",
    "        ]\n",
    "    }"
   ],
   "id": "7612f4e3aae4312d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:23:00.572401Z",
     "start_time": "2025-06-28T10:22:49.811606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 7. KOMBINASI SEMUA ===\n",
    "case_records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        meta = extract_metadata(row)\n",
    "        content = extract_fakta_dan_putusan(row)\n",
    "        feature = compute_features(row)\n",
    "        case = {**meta, **content, **feature}\n",
    "        case_records.append(case)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing case {idx + 1}: {e}\")"
   ],
   "id": "e5c3b600d7af3130",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:23:01.392262Z",
     "start_time": "2025-06-28T10:23:00.598008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 8. SIMPAN CSV DAN JSON ===\n",
    "output_folder = \"data/processed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Simpan ke CSV\n",
    "df_case = pd.DataFrame(case_records)\n",
    "csv_columns = [\n",
    "    \"case_id\", \"no_perkara\", \"tanggal\", \"ringkasan_fakta\", \"pasal\", \"pihak\", \"text_full\",\n",
    "    \"jenis_perkara\", \"hakim_ketua\", \"hakim_anggota\", \"panitera\", \"tahun\",\n",
    "    \"lembaga_peradilan\", \"jenis_lembaga_peradilan\", \"tanggal_musyawarah\",\n",
    "    \"tanggal_dibacakan\", \"pidana_penjara\", \"length\", \"qa_pair\"\n",
    "]\n",
    "df_case[csv_columns].to_csv(os.path.join(output_folder, \"cases.csv\"), index=False)\n",
    "\n",
    "# Simpan embeddings separately\n",
    "embeddings = [{\"case_id\": case[\"case_id\"], \"tfidf\": case[\"tfidf\"], \"indobert_embedding\": case[\"indobert_embedding\"]} for case in case_records]\n",
    "with open(os.path.join(output_folder, \"embeddings.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(embeddings, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Simpan ke JSON (full data)\n",
    "json_path = os.path.join(output_folder, \"cases.json\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(case_records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Case representation berhasil disimpan ke:\\n- CSV: {os.path.join(output_folder, 'cases.csv')}\\n- JSON: {json_path}\\n- Embeddings: {os.path.join(output_folder, 'embeddings.json')}\")"
   ],
   "id": "3e705ba1d8776835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Case representation berhasil disimpan ke:\n",
      "- CSV: data/processed\\cases.csv\n",
      "- JSON: data/processed\\cases.json\n",
      "- Embeddings: data/processed\\embeddings.json\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
