{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:30:25.463090Z",
     "start_time": "2025-06-28T06:30:22.840137Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers torch sentence-transformers Sastrawi scikit-learn faiss-cpu rank_bm25 matplotlib",
   "id": "a654bad1a7bf9182",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.11.0)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (0.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:30:33.399749Z",
     "start_time": "2025-06-28T06:30:25.540757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import faiss\n",
    "from scipy.stats import rankdata"
   ],
   "id": "b3a446d497cd4098",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:30:33.543405Z",
     "start_time": "2025-06-28T06:30:33.410282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 1. LOAD DATA ===\n",
    "file_path = 'data/processed/cases.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path} not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Ensure case_id is unique\n",
    "if df[\"case_id\"].duplicated().any():\n",
    "    print(\"Error: Duplicate case_id found in dataset.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check for required columns\n",
    "required_columns = ['case_id', 'text_full', 'pasal', 'pidana_penjara']\n",
    "if not all(col in df for col in required_columns):\n",
    "    print(f\"Error: Dataset missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
    "    sys.exit(1)"
   ],
   "id": "f51593a07c0a63cc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:30:33.572905Z",
     "start_time": "2025-06-28T06:30:33.558565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 2. EXTRAK SOLUSI ===\n",
    "def normalize_solution(text: str) -> str:\n",
    "    \"\"\"Normalize solution text to standardize penalties (e.g., 'penjara 2 tahun').\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"Unknown\"\n",
    "    text = text.lower().strip()\n",
    "    match = re.search(r'(penjara|denda)?\\s*(\\d+\\.?\\d*)\\s*(tahun|bulan|juta)?', text)\n",
    "    if match:\n",
    "        penalty_type = match.group(1) if match.group(1) else \"penjara\"\n",
    "        value = match.group(2)\n",
    "        unit = match.group(3) if match.group(3) else \"\"\n",
    "        return f\"{penalty_type} {value} {unit}\".strip()\n",
    "    return text\n",
    "\n",
    "# Create case_solutions dictionary\n",
    "case_solutions = {\n",
    "    row['case_id']: normalize_solution(str(row['pidana_penjara']))\n",
    "    for _, row in df.iterrows()\n",
    "    if pd.notna(row['pidana_penjara']) and str(row['pidana_penjara']).strip()\n",
    "}\n",
    "if len(case_solutions) < len(df):\n",
    "    print(f\"Warning: {len(df) - len(case_solutions)} cases with missing or empty pidana_penjara skipped.\")\n",
    "\n",
    "if not case_solutions:\n",
    "    print(\"Error: No valid solutions found in dataset.\")\n",
    "    sys.exit(1)"
   ],
   "id": "c23e6bc643d7b22d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:18.895558Z",
     "start_time": "2025-06-28T06:30:33.590339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 3. PREPROCESSING ===\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stop_words_indonesia = stopword_factory.get_stop_words() + [\"terdakwa\", \"korban\", \"menyatakan\", \"secara\", \"sah\", \"meyakinkan\"]\n",
    "\n",
    "synonyms = {\n",
    "    \"pengeroyokan\": [\"kekerasan bersama-sama\", \"penganiayaan bersama-sama\"],\n",
    "    \"penganiayaan\": [\"kekerasan\", \"penyerangan\"],\n",
    "    \"turut serta\": [\"ikut serta\", \"bersama-sama\"],\n",
    "    \"luka berat\": [\"cedera parah\", \"luka serius\"]\n",
    "}\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"Preprocess text: lowercase, remove punctuation, stem, apply synonyms, remove stopwords.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = stemmer.stem(text)\n",
    "    for key, syn_list in synonyms.items():\n",
    "        for syn in syn_list:\n",
    "            text = text.replace(syn, key)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words_indonesia])\n",
    "    return text\n",
    "\n",
    "corpus = df[\"text_full\"].apply(preprocess)"
   ],
   "id": "774cc2068cb51ef2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.437860Z",
     "start_time": "2025-06-28T06:31:18.926216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 4. REPRESENTASI VEKTOR ===\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_indonesia, max_features=15000, sublinear_tf=True, ngram_range=(1, 3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# IndoBERT Embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = AutoModel.from_pretrained(\"./indobert_finetuned\" if os.path.exists(\"./indobert_finetuned\") else \"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "def bert_embed(text: str) -> np.ndarray:\n",
    "    \"\"\"Generate IndoBERT embeddings for text.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.max(dim=1).values.squeeze().numpy()\n",
    "        return embeddings / np.linalg.norm(embeddings) if np.linalg.norm(embeddings) != 0 else np.zeros(768)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {e}\")\n",
    "        return np.zeros(768)\n",
    "\n",
    "# Load or compute IndoBERT embeddings\n",
    "embeddings_file = 'data/processed/embeddings.json'\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    bert_embeddings = np.array([data[\"indobert_embedding\"] for data in embeddings_data])\n",
    "else:\n",
    "    bert_embeddings = np.array([bert_embed(text) for text in corpus])"
   ],
   "id": "b987461413658f0a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.489286Z",
     "start_time": "2025-06-28T06:31:22.463280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 5. SPLITTING DATA ===\n",
    "nb_trained = True\n",
    "stratify_col = df[\"pasal\"]\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, df[\"pasal\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    ")\n",
    "X_train_bert, X_test_bert = train_test_split(\n",
    "    bert_embeddings, test_size=0.2, random_state=42, stratify=stratify_col\n",
    ")\n",
    "case_id_train, case_id_test = train_test_split(\n",
    "    df[\"case_id\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    ")\n",
    "text_test = df.iloc[case_id_test.index][\"text_full\"].apply(preprocess)\n",
    "pidana_test = df.iloc[case_id_test.index][\"pidana_penjara\"].apply(normalize_solution)"
   ],
   "id": "810ee2a50743e474",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.529924Z",
     "start_time": "2025-06-28T06:31:22.517524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 6. MODEL RETRIEVAL: NAIVE BAYES ===\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "y_pred = nb.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "prob_scores = nb.predict_proba(X_test_tfidf)\n",
    "class_labels = np.unique(y_train)\n",
    "log_losses = []\n",
    "for i, y_true in enumerate(y_test):\n",
    "    if y_true not in class_labels:\n",
    "        print(f\"Warning: Test pasal {y_true} not in training set. Skipping.\")\n",
    "        continue\n",
    "    true_class_idx = np.where(class_labels == y_true)[0][0]\n",
    "    true_prob = prob_scores[i, true_class_idx]\n",
    "    loss = -np.log(max(true_prob, 1e-15))\n",
    "    log_losses.append(loss)\n",
    "log_loss_value = np.mean(log_losses) if log_losses else 0.0\n",
    "print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Naive Bayes Log Loss: {log_loss_value:.4f}\")"
   ],
   "id": "747ae9c0d7d9779c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 1.0000\n",
      "Naive Bayes Log Loss: 0.0339\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.578228Z",
     "start_time": "2025-06-28T06:31:22.573463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 7. FAISS INDEX FOR INDOBERT RETRIEVAL ===\n",
    "dimension = bert_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(bert_embeddings.astype(np.float32))"
   ],
   "id": "9a5f3b7065a87249",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.629910Z",
     "start_time": "2025-06-28T06:31:22.621373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 8. FUNGSI RETRIEVAL ===\n",
    "def retrieve(query: str, k: int = 5, method: str = \"nb\") -> List[Tuple[int, float]]:\n",
    "    \"\"\"Retrieve top-k cases for a query using the specified method.\"\"\"\n",
    "    query_clean = preprocess(query)\n",
    "    try:\n",
    "        if method == \"bm25\":\n",
    "            query_tokens = query_clean.split()\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            top_k_idx = np.argsort(scores)[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"tfidf\":\n",
    "            query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "            scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            top_k_idx = scores.argsort()[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"bert\":\n",
    "            query_vec = bert_embed(query_clean).astype(np.float32)\n",
    "            query_vec = query_vec / np.linalg.norm(query_vec) if np.linalg.norm(query_vec) != 0 else np.zeros(dimension, dtype=np.float32)\n",
    "            scores, indices = index.search(query_vec.reshape(1, -1), k)\n",
    "            top_k_idx = indices[0]\n",
    "            scores = scores[0]\n",
    "        elif method == \"nb\":\n",
    "            query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "            predicted_pasal = nb.predict(query_vec)[0]\n",
    "            candidate_idx = df[df[\"pasal\"] == predicted_pasal].index\n",
    "            if len(candidate_idx) == 0:\n",
    "                print(f\"Warning: No cases found for predicted pasal {predicted_pasal}. Falling back to TF-IDF.\")\n",
    "                scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                top_k_idx = scores.argsort()[::-1][:k]\n",
    "                scores = scores[top_k_idx]\n",
    "            else:\n",
    "                candidate_matrix = tfidf_matrix[candidate_idx]\n",
    "                scores = cosine_similarity(query_vec, candidate_matrix).flatten()\n",
    "                top_k_idx = candidate_idx[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "                scores = scores[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'bm25', 'tfidf', 'bert', or 'nb'.\")\n",
    "        top_case_ids = df.iloc[top_k_idx][\"case_id\"].tolist()\n",
    "        return list(zip(top_case_ids, scores))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval for query '{query}' with method '{method}': {e}\")\n",
    "        return []"
   ],
   "id": "95ca5a916a616d47",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.676059Z",
     "start_time": "2025-06-28T06:31:22.669057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 9. PREDIKSI OUTCOME ===\n",
    "def predict_outcome(query: str, k: int = 5, method: str = \"nb\", strategy: str = \"weighted\") -> str:\n",
    "    \"\"\"Predict the outcome for a query based on top-k similar cases.\"\"\"\n",
    "    top_k = retrieve(query, k=k, method=method)\n",
    "    if not top_k:\n",
    "        return \"No similar cases found.\"\n",
    "    case_ids, scores = zip(*top_k)\n",
    "    solutions = [case_solutions[cid] for cid in case_ids if cid in case_solutions]\n",
    "    if not solutions:\n",
    "        return \"No solutions found for retrieved cases.\"\n",
    "    if strategy == \"majority\":\n",
    "        solution_counts = Counter(solutions)\n",
    "        predicted_solution = solution_counts.most_common(1)[0][0]\n",
    "    else:  # Weighted similarity\n",
    "        solution_scores = {}\n",
    "        for sol, score in zip(solutions, scores):\n",
    "            solution_scores[sol] = solution_scores.get(sol, 0.0) + score\n",
    "        predicted_solution = max(solution_scores, key=solution_scores.get)\n",
    "    return predicted_solution"
   ],
   "id": "3f551be1f8182d38",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.726663Z",
     "start_time": "2025-06-28T06:31:22.716868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 10. EVALUASI RETRIEVAL ===\n",
    "def eval_retrieval(queries: List[str], ground_truth: List[str], k: int = 5, method: str = \"nb\") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval performance for a list of queries.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    failure_cases = []\n",
    "\n",
    "    for query, gt_pasal in zip(queries, ground_truth):\n",
    "        top_k = retrieve(query, k=k, method=method)\n",
    "        if not top_k:\n",
    "            y_true.append(1)  # Assume relevant\n",
    "            y_pred.append(0)  # No cases retrieved\n",
    "            failure_cases.append({\"query\": query, \"ground_truth\": gt_pasal, \"top_k_case_ids\": [], \"reason\": \"No cases retrieved\"})\n",
    "            continue\n",
    "\n",
    "        case_ids, _ = zip(*top_k)\n",
    "        # Check if retrieved cases match ground-truth pasal\n",
    "        retrieved_pasals = [df[df[\"case_id\"] == cid][\"pasal\"].iloc[0] for cid in case_ids if cid in df[\"case_id\"].values]\n",
    "        relevant = [1 if pasal == gt_pasal else 0 for pasal in retrieved_pasals]\n",
    "\n",
    "        # If no relevant cases, log failure\n",
    "        if sum(relevant) == 0:\n",
    "            failure_cases.append({\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": gt_pasal,\n",
    "                \"top_k_case_ids\": list(case_ids),\n",
    "                \"reason\": \"No relevant cases retrieved\"\n",
    "            })\n",
    "\n",
    "        # For accuracy, consider retrieval correct if at least one case is relevant\n",
    "        y_true.append(1)  # Query has relevant cases in dataset\n",
    "        y_pred.append(1 if any(relevant) else 0)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred) if y_true else 0.0,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Save failure cases\n",
    "    with open(f\"data/eval/failure_cases_retrieval_{method}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(failure_cases, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return metrics"
   ],
   "id": "fe70f133527e242d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.770143Z",
     "start_time": "2025-06-28T06:31:22.764168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 11. EVALUASI PREDICTION ===\n",
    "def eval_prediction(queries: List[str], ground_truth: List[str], k: int = 5, method: str = \"nb\", strategy: str = \"weighted\") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate prediction performance for a list of queries.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    failure_cases = []\n",
    "\n",
    "    for query, gt_solution in zip(queries, ground_truth):\n",
    "        if gt_solution == \"Unknown\":\n",
    "            continue  # Skip queries with invalid ground-truth\n",
    "        predicted_solution = predict_outcome(query, k=k, method=method, strategy=strategy)\n",
    "        if predicted_solution in [\"No similar cases found\", \"No solutions found for retrieved cases\"]:\n",
    "            y_true.append(1)  # Assume relevant\n",
    "            y_pred.append(0)  # Prediction failed\n",
    "            failure_cases.append({\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": gt_solution,\n",
    "                \"predicted\": predicted_solution,\n",
    "                \"reason\": predicted_solution\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        y_true.append(1)  # Query has a valid ground-truth\n",
    "        y_pred.append(1 if predicted_solution == gt_solution else 0)\n",
    "        if predicted_solution != gt_solution:\n",
    "            failure_cases.append({\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": gt_solution,\n",
    "                \"predicted\": predicted_solution,\n",
    "                \"reason\": \"Prediction mismatch\"\n",
    "            })\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred) if y_true else 0.0,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Save failure cases\n",
    "    with open(f\"data/eval/failure_cases_prediction_{method}_{strategy}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(failure_cases, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return metrics"
   ],
   "id": "d4c63d7d2af12b9c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:22.813446Z",
     "start_time": "2025-06-28T06:31:22.806849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 12. VISUALISASI & LAPORAN ===\n",
    "def create_metrics_table(retrieval_metrics: Dict[str, Dict], prediction_metrics: Dict[str, Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Create a table of metrics for all models.\"\"\"\n",
    "    data = []\n",
    "    for method in retrieval_metrics:\n",
    "        metrics = retrieval_metrics[method]\n",
    "        data.append({\n",
    "            \"Model\": method,\n",
    "            \"Type\": \"Retrieval\",\n",
    "            \"Accuracy\": metrics[\"accuracy\"],\n",
    "            \"Precision\": metrics[\"precision\"],\n",
    "            \"Recall\": metrics[\"recall\"],\n",
    "            \"F1-Score\": metrics[\"f1_score\"]\n",
    "        })\n",
    "    for key, metrics in prediction_metrics.items():\n",
    "        method, strategy = key.split(\"_\")\n",
    "        data.append({\n",
    "            \"Model\": f\"{method}_{strategy}\",\n",
    "            \"Type\": \"Prediction\",\n",
    "            \"Accuracy\": metrics[\"accuracy\"],\n",
    "            \"Precision\": metrics[\"precision\"],\n",
    "            \"Recall\": metrics[\"recall\"],\n",
    "            \"F1-Score\": metrics[\"f1_score\"]\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def plot_metrics(metrics_df: pd.DataFrame):\n",
    "    \"\"\"Plot bar chart of F1-scores for retrieval and prediction.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    models = metrics_df[\"Model\"]\n",
    "    f1_scores = metrics_df[\"F1-Score\"]\n",
    "    colors = [\"blue\" if t == \"Retrieval\" else \"green\" for t in metrics_df[\"Type\"]]\n",
    "\n",
    "    plt.bar(models, f1_scores, color=colors)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(\"F1-Score Comparison for Retrieval and Prediction\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(\"data/eval\", exist_ok=True)\n",
    "    plt.savefig(\"data/eval/f1_score_comparison.png\")\n",
    "    plt.close()"
   ],
   "id": "875800a3c6d183c1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T06:31:30.254248Z",
     "start_time": "2025-06-28T06:31:22.858608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 13. MAIN EVALUATION ===\n",
    "# Prepare test queries and ground-truth\n",
    "queries = text_test.tolist()\n",
    "pasal_ground_truth = y_test.tolist()\n",
    "solution_ground_truth = pidana_test.tolist()\n",
    "\n",
    "# Evaluate retrieval for each method\n",
    "retrieval_methods = [\"nb\", \"tfidf\", \"bert\", \"bm25\"]\n",
    "k = 5\n",
    "retrieval_metrics = {}\n",
    "for method in retrieval_methods:\n",
    "    metrics = eval_retrieval(queries, pasal_ground_truth, k=k, method=method)\n",
    "    retrieval_metrics[method] = metrics\n",
    "    print(f\"Retrieval Metrics for {method}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Evaluate prediction for each method and strategy\n",
    "prediction_configs = [(method, strategy) for method in retrieval_methods for strategy in [\"weighted\", \"majority\"]]\n",
    "prediction_metrics = {}\n",
    "for method, strategy in prediction_configs:\n",
    "    metrics = eval_prediction(queries, solution_ground_truth, k=k, method=method, strategy=strategy)\n",
    "    prediction_metrics[f\"{method}_{strategy}\"] = metrics\n",
    "    print(f\"Prediction Metrics for {method} ({strategy}):\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Create and save metrics table\n",
    "metrics_df = create_metrics_table(retrieval_metrics, prediction_metrics)\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "metrics_df.to_csv(\"data/eval/retrieval_metrics.csv\" if metrics_df[\"Type\"].str.contains(\"Retrieval\").any() else \"data/eval/prediction_metrics.csv\", index=False)\n",
    "print(\"Metrics saved to data/eval/retrieval_metrics.csv and data/eval/prediction_metrics.csv\")\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(metrics_df)\n",
    "print(\"F1-Score comparison plot saved to data/eval/f1_score_comparison.png\")"
   ],
   "id": "c106ecc829b2c7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics for nb:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "---\n",
      "Retrieval Metrics for tfidf:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "---\n",
      "Retrieval Metrics for bert:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "---\n",
      "Retrieval Metrics for bm25:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "---\n",
      "Prediction Metrics for nb (weighted):\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "---\n",
      "Prediction Metrics for nb (majority):\n",
      "Accuracy: 0.6667\n",
      "Precision: 1.0000\n",
      "Recall: 0.6667\n",
      "F1-Score: 0.8000\n",
      "---\n",
      "Prediction Metrics for tfidf (weighted):\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "---\n",
      "Prediction Metrics for tfidf (majority):\n",
      "Accuracy: 0.6667\n",
      "Precision: 1.0000\n",
      "Recall: 0.6667\n",
      "F1-Score: 0.8000\n",
      "---\n",
      "Prediction Metrics for bert (weighted):\n",
      "Accuracy: 0.8889\n",
      "Precision: 1.0000\n",
      "Recall: 0.8889\n",
      "F1-Score: 0.9412\n",
      "---\n",
      "Prediction Metrics for bert (majority):\n",
      "Accuracy: 0.8889\n",
      "Precision: 1.0000\n",
      "Recall: 0.8889\n",
      "F1-Score: 0.9412\n",
      "---\n",
      "Prediction Metrics for bm25 (weighted):\n",
      "Accuracy: 0.8889\n",
      "Precision: 1.0000\n",
      "Recall: 0.8889\n",
      "F1-Score: 0.9412\n",
      "---\n",
      "Prediction Metrics for bm25 (majority):\n",
      "Accuracy: 0.7778\n",
      "Precision: 1.0000\n",
      "Recall: 0.7778\n",
      "F1-Score: 0.8750\n",
      "---\n",
      "Metrics saved to data/eval/retrieval_metrics.csv and data/eval/prediction_metrics.csv\n",
      "F1-Score comparison plot saved to data/eval/f1_score_comparison.png\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
