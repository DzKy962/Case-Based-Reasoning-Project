{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:07.216841Z",
     "start_time": "2025-06-28T12:23:05.171849Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers torch sentence-transformers Sastrawi scikit-learn faiss-cpu rank_bm25",
   "id": "1fa0399517dd5136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.11.0)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (0.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:07.250336Z",
     "start_time": "2025-06-28T12:23:07.246279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import faiss\n",
    "from scipy.stats import rankdata"
   ],
   "id": "4e478bceb13c3e61",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:07.292034Z",
     "start_time": "2025-06-28T12:23:07.282190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 1. LOAD DATA ===\n",
    "file_path = 'data/processed/cases.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path} not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Ensure case_id is unique\n",
    "if df[\"case_id\"].duplicated().any():\n",
    "    print(\"Error: Duplicate case_id found in dataset.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check for required columns\n",
    "required_columns = ['case_id', 'text_full', 'pidana_penjara']\n",
    "if not all(col in df for col in required_columns):\n",
    "    print(f\"Error: Dataset missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
    "    sys.exit(1)"
   ],
   "id": "ff12037f37ec510a",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:07.325410Z",
     "start_time": "2025-06-28T12:23:07.317410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 2. EXTRAK SOLUSI ===\n",
    "def normalize_solution(text: str) -> str:\n",
    "    if not isinstance(text, str) or not text.strip() or text.lower() in ['none', 'tidak dipidana', 'bebas', 'tidak terbukti', 'lepas']:\n",
    "        return \"None\"\n",
    "    text = text.lower().strip()\n",
    "    match = re.search(r'(penjara|denda)?\\s*(\\d+\\.?\\d*)\\s*(tahun|bulan|juta)?(?:\\s*(\\d+)\\s*bulan)?', text)\n",
    "    if match:\n",
    "        penalty_type = match.group(1) if match.group(1) else \"penjara\"\n",
    "        value = match.group(2)\n",
    "        unit = match.group(3) if match.group(3) else \"tahun\" if penalty_type == \"penjara\" else \"juta\"\n",
    "        extra_months = f\" {match.group(4)} bulan\" if match.group(4) else \"\"\n",
    "        return f\"{penalty_type} {value} {unit}{extra_months}\".strip()\n",
    "    return \"None\"\n",
    "\n",
    "# Create case_solutions dictionary\n",
    "case_solutions = {\n",
    "    row['case_id']: normalize_solution(str(row['pidana_penjara']))\n",
    "    for _, row in df.iterrows()\n",
    "}\n",
    "if len(case_solutions) < len(df):\n",
    "    print(f\"Warning: {len(df) - len(case_solutions)} cases with missing or empty pidana_penjara.\")\n",
    "\n",
    "if not case_solutions:\n",
    "    print(\"Error: No valid solutions found in dataset.\")\n",
    "    sys.exit(1)"
   ],
   "id": "ec7a4d1544b25eb4",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:44.801793Z",
     "start_time": "2025-06-28T12:23:07.350976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 3. PREPROCESSING ===\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stop_words_indonesia = stopword_factory.get_stop_words() + [\"terdakwa\", \"korban\", \"menyatakan\", \"secara\", \"sah\", \"meyakinkan\"]\n",
    "\n",
    "synonyms = {\n",
    "    \"pengeroyokan\": [\"kekerasan bersama-sama\", \"penganiayaan bersama-sama\"],\n",
    "    \"penganiayaan\": [\"kekerasan\", \"penyerangan\"],\n",
    "    \"turut serta\": [\"ikut serta\", \"bersama-sama\"],\n",
    "    \"luka berat\": [\"cedera parah\", \"luka serius\"],\n",
    "    \"penganiayaan ringan\": [\"kekerasan ringan\", \"penyerangan ringan\"]\n",
    "}\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"Preprocess text: lowercase, remove punctuation, stem, apply synonyms, remove stopwords.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = stemmer.stem(text)\n",
    "    for key, syn_list in synonyms.items():\n",
    "        for syn in syn_list:\n",
    "            text = text.replace(syn, key)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words_indonesia])\n",
    "    return text\n",
    "\n",
    "corpus = df[\"text_full\"].apply(preprocess)"
   ],
   "id": "1c931f15b75ea1de",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.360860Z",
     "start_time": "2025-06-28T12:23:44.824965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 4. REPRESENTASI VEKTOR ===\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_indonesia, max_features=15000, sublinear_tf=True, ngram_range=(1, 3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# IndoBERT Embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = AutoModel.from_pretrained(\"./indobert_finetuned\" if os.path.exists(\"./indobert_finetuned\") else \"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "def bert_embed(text: str) -> np.ndarray:\n",
    "    \"\"\"Generate IndoBERT embeddings for text.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.max(dim=1).values.squeeze().numpy()\n",
    "        return embeddings / np.linalg.norm(embeddings) if np.linalg.norm(embeddings) != 0 else np.zeros(768)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {e}\")\n",
    "        return np.zeros(768)\n",
    "\n",
    "# Load or compute IndoBERT embeddings\n",
    "embeddings_file = 'data/processed/embeddings.json'\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    bert_embeddings = np.array([data[\"indobert_embedding\"] for data in embeddings_data])\n",
    "else:\n",
    "    bert_embeddings = np.array([bert_embed(text) for text in corpus])"
   ],
   "id": "6b837330626e327d",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.391189Z",
     "start_time": "2025-06-28T12:23:48.383141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 5. SPLITTING DATA ===\n",
    "if \"pasal\" not in df:\n",
    "    print(\"Warning: 'pasal' column not found. Skipping Naive Bayes training.\")\n",
    "    nb_trained = False\n",
    "    stratify_col = None\n",
    "    X_train_tfidf, X_test_tfidf, case_id_train, case_id_test = train_test_split(\n",
    "        tfidf_matrix, df[\"case_id\"], test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train_bert, X_test_bert = train_test_split(\n",
    "        bert_embeddings, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    nb_trained = True\n",
    "    stratify_col = df[\"pasal\"]\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "        tfidf_matrix, df[\"pasal\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )\n",
    "    X_train_bert, X_test_bert = train_test_split(\n",
    "        bert_embeddings, test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )\n",
    "    case_id_train, case_id_test = train_test_split(\n",
    "        df[\"case_id\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )"
   ],
   "id": "d305813fdd54d8b4",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.427074Z",
     "start_time": "2025-06-28T12:23:48.419252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 6. MODEL RETRIEVAL: NAIVE BAYES ===\n",
    "if nb_trained:\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_tfidf, y_train)\n",
    "    y_pred = nb.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    prob_scores = nb.predict_proba(X_test_tfidf)\n",
    "    class_labels = np.unique(y_train)\n",
    "    log_losses = []\n",
    "    for i, y_true in enumerate(y_test):\n",
    "        if y_true not in class_labels:\n",
    "            print(f\"Warning: Test pasal {y_true} not in training set. Skipping.\")\n",
    "            continue\n",
    "        true_class_idx = np.where(class_labels == y_true)[0][0]\n",
    "        true_prob = prob_scores[i, true_class_idx]\n",
    "        loss = -np.log(max(true_prob, 1e-15))\n",
    "        log_losses.append(loss)\n",
    "    log_loss_value = np.mean(log_losses) if log_losses else 0.0\n",
    "    print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Naive Bayes Log Loss: {log_loss_value:.4f}\")\n",
    "else:\n",
    "    print(\"Naive Bayes training skipped due to missing 'pasal' column.\")"
   ],
   "id": "9cbb7da88fc42fdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 1.0000\n",
      "Naive Bayes Log Loss: 0.0339\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.468667Z",
     "start_time": "2025-06-28T12:23:48.465256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 7. FAISS INDEX FOR INDOBERT RETRIEVAL ===\n",
    "dimension = bert_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(bert_embeddings.astype(np.float32))"
   ],
   "id": "444f5152bb59956a",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.509282Z",
     "start_time": "2025-06-28T12:23:48.498946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 8. FUNGSI RETRIEVAL ===\n",
    "def retrieve(query: str, k: int = 5, method: str = \"nb\") -> List[Tuple[int, float]]:\n",
    "    \"\"\"Retrieve top-k cases for a query using the specified method.\"\"\"\n",
    "    query_clean = preprocess(query)\n",
    "    try:\n",
    "        if method == \"bm25\":\n",
    "            query_tokens = query_clean.split()\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            # Normalize BM25 scores to [0,1] for consistency\n",
    "            scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores) + 1e-10)\n",
    "            top_k_idx = np.argsort(scores)[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"tfidf\":\n",
    "            query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "            scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            top_k_idx = scores.argsort()[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"bert\":\n",
    "            query_vec = bert_embed(query_clean).astype(np.float32)\n",
    "            query_vec = query_vec / np.linalg.norm(query_vec) if np.linalg.norm(query_vec) != 0 else np.zeros(dimension, dtype=np.float32)\n",
    "            scores, indices = index.search(query_vec.reshape(1, -1), k)\n",
    "            top_k_idx = indices[0]\n",
    "            scores = scores[0]\n",
    "        elif method == \"nb\":\n",
    "            if not nb_trained:\n",
    "                print(\"Warning: Naive Bayes not trained. Falling back to TF-IDF.\")\n",
    "                query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "                scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                top_k_idx = scores.argsort()[::-1][:k]\n",
    "                scores = scores[top_k_idx]\n",
    "            else:\n",
    "                query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "                predicted_pasal = nb.predict(query_vec)[0]\n",
    "                candidate_idx = df[df[\"pasal\"] == predicted_pasal].index\n",
    "                if len(candidate_idx) == 0:\n",
    "                    print(f\"Warning: No cases found for predicted pasal {predicted_pasal}. Falling back to TF-IDF.\")\n",
    "                    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                    top_k_idx = scores.argsort()[::-1][:k]\n",
    "                    scores = scores[top_k_idx]\n",
    "                else:\n",
    "                    candidate_matrix = tfidf_matrix[candidate_idx]\n",
    "                    scores = cosine_similarity(query_vec, candidate_matrix).flatten()\n",
    "                    top_k_idx = candidate_idx[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "                    scores = scores[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'bm25', 'tfidf', 'bert', or 'nb'.\")\n",
    "        top_case_ids = df.iloc[top_k_idx][\"case_id\"].tolist()\n",
    "        return list(zip(top_case_ids, scores))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval for query '{query}' with method '{method}': {e}\")\n",
    "        return []"
   ],
   "id": "99f94c9f2518c302",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.539157Z",
     "start_time": "2025-06-28T12:23:48.532623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 9. PREDIKSI OUTCOME ===\n",
    "def predict_outcome(query: str, k: int = 5, method: str = \"nb\", strategy: str = \"weighted\", threshold: float = 0.1) -> str:\n",
    "    \"\"\"\n",
    "    Predict the outcome for a query based on top-k similar cases.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query describing the case.\n",
    "        k (int): Number of top cases to retrieve.\n",
    "        method (str): Retrieval method ('bm25', 'tfidf', 'bert', 'nb').\n",
    "        strategy (str): Prediction strategy ('majority', 'weighted').\n",
    "        threshold (float): Minimum similarity score to consider a case relevant.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted solution or \"None\" if no penalty is inferred.\n",
    "    \"\"\"\n",
    "    top_k = retrieve(query, k=k, method=method)\n",
    "    if not top_k:\n",
    "        return \"None\"  # No similar cases found\n",
    "\n",
    "    case_ids, scores = zip(*top_k)\n",
    "    # Filter cases with similarity scores above threshold\n",
    "    valid_cases = [(cid, score) for cid, score in top_k if score >= threshold]\n",
    "    if not valid_cases:\n",
    "        return \"None\"  # No cases meet the threshold\n",
    "\n",
    "    case_ids, scores = zip(*valid_cases)\n",
    "    solutions = [case_solutions[cid] for cid in case_ids if cid in case_solutions]\n",
    "\n",
    "    if not solutions:\n",
    "        return \"None\"  # No valid solutions found\n",
    "\n",
    "    # Check if all retrieved cases have \"None\" as solution\n",
    "    if all(sol == \"None\" for sol in solutions):\n",
    "        return \"None\"\n",
    "\n",
    "    # Apply prediction strategy\n",
    "    if strategy == \"majority\":\n",
    "        solution_counts = Counter(solutions)\n",
    "        predicted_solution = solution_counts.most_common(1)[0][0]\n",
    "    else:  # Weighted similarity\n",
    "        solution_scores = {}\n",
    "        total_score = sum(scores)\n",
    "        normalized_scores = [score / (total_score + 1e-10) for score in scores]  # Normalize scores\n",
    "        for sol, score in zip(solutions, normalized_scores):\n",
    "            if sol != \"None\":  # Exclude \"None\" from weighted scoring\n",
    "                solution_scores[sol] = solution_scores.get(sol, 0.0) + score\n",
    "        if not solution_scores:\n",
    "            return \"None\"  # No non-None solutions\n",
    "        predicted_solution = max(solution_scores, key=solution_scores.get)\n",
    "\n",
    "    return predicted_solution"
   ],
   "id": "92d70bee8675ca2d",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.627519Z",
     "start_time": "2025-06-28T12:23:48.572036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 10. DEMO MANUAL ===\n",
    "# Configuration\n",
    "RETRIEVAL_METHOD = \"tfidf\"  # \"nb\" NaiveBayes, \"tfidf\" TF-IDF, \"bert\" IndoBERT, and \"bm25\" BM25\n",
    "PREDICTION_STRATEGY = \"majority\"  # Weighted or majority for better scores\n",
    "SIMILARITY_THRESHOLD = 0.1  # Minimum: 0.0 (includes all cases, regardless of similarity).Maximum: 1.0 (requires perfect similarity, rarely achieved).\n",
    "\n",
    "# Define new queries with some known ground truths for evaluation\n",
    "new_queries = [\n",
    "    {\"query_id\": 1, \"query\": \"Terdakwa melakukan penganiayaan ringan tanpa luka di tempat umum\", \"ground_truth\": \"penjara 1 tahun\"},\n",
    "    {\"query_id\": 2, \"query\": \"Terdakwa bersama-sama melakukan pengeroyokan menyebabkan luka berat\", \"ground_truth\": \"penjara 3 tahun\"},\n",
    "    {\"query_id\": 3, \"query\": \"Terdakwa melakukan penggelapan dalam jabatan senilai 10 juta\", \"ground_truth\": \"penjara 1 tahun\"},\n",
    "    {\"query_id\": 4, \"query\": \"Terdakwa menghasut orang untuk melakukan kekerasan\", \"ground_truth\": \"penjara 2 tahun\"},\n",
    "    {\"query_id\": 5, \"query\": \"Terdakwa melakukan kekerasan berat terhadap anak di bawah umur\", \"ground_truth\": \"penjara 5 tahun\"},\n",
    "    {\"query_id\": 6, \"query\": \"Terdakwa membantu tetangga menyeberang jalan\", \"ground_truth\": \"None\"}\n",
    "]\n",
    "\n",
    "# Run predictions and store results\n",
    "results = []\n",
    "exact_matches = 0\n",
    "total_with_ground_truth = 0\n",
    "\n",
    "with open(\"logs.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(f\"\\n=== Prediction Run on {pd.Timestamp.now()} ===\\n\")\n",
    "    log_file.write(f\"Method: {RETRIEVAL_METHOD.upper()}, Strategy: {PREDICTION_STRATEGY}, Threshold: {SIMILARITY_THRESHOLD}\\n\")\n",
    "\n",
    "    for q in new_queries:\n",
    "        query_id = q[\"query_id\"]\n",
    "        query = q[\"query\"]\n",
    "        ground_truth = q[\"ground_truth\"]\n",
    "\n",
    "        # Predict outcome\n",
    "        predicted_solution = predict_outcome(query, k=5, method=RETRIEVAL_METHOD, strategy=PREDICTION_STRATEGY, threshold=SIMILARITY_THRESHOLD)\n",
    "        top_k = retrieve(query, k=5, method=RETRIEVAL_METHOD)\n",
    "        top_case_ids = [cid for cid, _ in top_k]\n",
    "        top_scores = [score for _, score in top_k]\n",
    "\n",
    "        # Evaluate if ground truth exists\n",
    "        is_correct = False\n",
    "        if ground_truth is not None:\n",
    "            total_with_ground_truth += 1\n",
    "            normalized_ground_truth = normalize_solution(ground_truth)\n",
    "            is_correct = normalized_ground_truth == predicted_solution\n",
    "            if is_correct:\n",
    "                exact_matches += 1\n",
    "        else:\n",
    "            normalized_ground_truth = predicted_solution  # Use predicted as ground truth if None\n",
    "\n",
    "        # Log and print results\n",
    "        print(f\"Query ID: {query_id}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Predicted Solution: {predicted_solution}\")\n",
    "        print(f\"Top-5 Case IDs: {top_case_ids}\")\n",
    "        print(f\"Scores: {top_scores}\")\n",
    "        print(f\"Ground Truth: {'Not available' if ground_truth is None else ground_truth}\")\n",
    "        print(f\"Match: {'Yes' if is_correct else 'No'}\")\n",
    "        print(\"---\")\n",
    "\n",
    "        log_file.write(f\"Query ID: {query_id}\\n\")\n",
    "        log_file.write(f\"Query: {query}\\n\")\n",
    "        log_file.write(f\"Predicted Solution: {predicted_solution}\\n\")\n",
    "        log_file.write(f\"Top-5 Case IDs: {top_case_ids}\\n\")\n",
    "        log_file.write(f\"Scores: {top_scores}\\n\")\n",
    "        log_file.write(f\"Ground Truth: {'Not available' if ground_truth is None else ground_truth}\\n\")\n",
    "        log_file.write(f\"Match: {'Yes' if is_correct else 'No'}\\n\")\n",
    "        log_file.write(\"---\\n\")\n",
    "\n",
    "        results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"query\": query,\n",
    "            \"predicted_solution\": predicted_solution,\n",
    "            \"top_5_case_ids\": \",\".join(map(str, top_case_ids)),\n",
    "            \"scores\": \",\".join([f\"{score:.6f}\" for score in top_scores]),\n",
    "            \"ground_truth\": ground_truth if ground_truth is not None else predicted_solution\n",
    "        })\n",
    "\n",
    "# Calculate and log accuracy\n",
    "if total_with_ground_truth > 0:\n",
    "    accuracy = exact_matches / total_with_ground_truth\n",
    "    print(f\"Exact Match Accuracy: {accuracy:.4f} ({exact_matches}/{total_with_ground_truth})\")\n",
    "    with open(\"logs.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f\"Exact Match Accuracy: {accuracy:.4f} ({exact_matches}/{total_with_ground_truth})\\n\")\n",
    "else:\n",
    "    print(\"No ground-truth solutions available for accuracy calculation.\")\n",
    "    with open(\"logs.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(\"No ground-truth solutions available for accuracy calculation.\\n\")"
   ],
   "id": "e1ebf5628229b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1\n",
      "Query: Terdakwa melakukan penganiayaan ringan tanpa luka di tempat umum\n",
      "Predicted Solution: penjara 1 tahun\n",
      "Top-5 Case IDs: [7, 64, 28, 5, 66]\n",
      "Scores: [0.11627914708182566, 0.10422558440011195, 0.09124974772771396, 0.09001615884504785, 0.08989948185598731]\n",
      "Ground Truth: penjara 1 tahun\n",
      "Match: Yes\n",
      "---\n",
      "Query ID: 2\n",
      "Query: Terdakwa bersama-sama melakukan pengeroyokan menyebabkan luka berat\n",
      "Predicted Solution: penjara 3 tahun\n",
      "Top-5 Case IDs: [82, 75, 28, 76, 14]\n",
      "Scores: [0.11161688265221262, 0.10907609926741638, 0.10408386945942408, 0.10240657708778028, 0.08754543870021377]\n",
      "Ground Truth: penjara 3 tahun\n",
      "Match: Yes\n",
      "---\n",
      "Query ID: 3\n",
      "Query: Terdakwa melakukan penggelapan dalam jabatan senilai 10 juta\n",
      "Predicted Solution: None\n",
      "Top-5 Case IDs: [6, 8, 13, 36, 21]\n",
      "Scores: [0.07957304682778768, 0.0777022458661865, 0.05855179152175507, 0.0536419993361455, 0.052036664795012615]\n",
      "Ground Truth: penjara 1 tahun\n",
      "Match: No\n",
      "---\n",
      "Query ID: 4\n",
      "Query: Terdakwa menghasut orang untuk melakukan kekerasan\n",
      "Predicted Solution: penjara 2 tahun\n",
      "Top-5 Case IDs: [6, 30, 8, 76, 80]\n",
      "Scores: [0.10009523484699787, 0.09843751516825008, 0.09774194728206748, 0.09544382390694385, 0.09426330746174853]\n",
      "Ground Truth: penjara 2 tahun\n",
      "Match: Yes\n",
      "---\n",
      "Query ID: 5\n",
      "Query: Terdakwa melakukan kekerasan berat terhadap anak di bawah umur\n",
      "Predicted Solution: penjara 2 tahun\n",
      "Top-5 Case IDs: [32, 82, 80, 50, 6]\n",
      "Scores: [0.13225875577366206, 0.10096044725633323, 0.08288693289139623, 0.08218672481272436, 0.08105714140124065]\n",
      "Ground Truth: penjara 5 tahun\n",
      "Match: No\n",
      "---\n",
      "Query ID: 6\n",
      "Query: Terdakwa membantu tetangga menyeberang jalan\n",
      "Predicted Solution: penjara 1 tahun\n",
      "Top-5 Case IDs: [60, 53, 18, 39, 88]\n",
      "Scores: [0.13329829242427238, 0.12158663478021185, 0.11728664946399794, 0.11247923416020986, 0.10443005897300696]\n",
      "Ground Truth: None\n",
      "Match: No\n",
      "---\n",
      "Exact Match Accuracy: 0.5000 (3/6)\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:23:48.739574Z",
     "start_time": "2025-06-28T12:23:48.732539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save results\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"data/results/predictions.csv\", index=False)\n",
    "print(f\"Results saved to data/results/predictions.csv\")"
   ],
   "id": "ebaf0cabd4170ffe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data/results/predictions.csv\n"
     ]
    }
   ],
   "execution_count": 126
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
