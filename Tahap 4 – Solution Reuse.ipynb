{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-28T10:24:48.642344Z",
     "start_time": "2025-06-28T10:24:46.103380Z"
    }
   },
   "source": "!pip install transformers torch sentence-transformers Sastrawi scikit-learn faiss-cpu rank_bm25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.11.0)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (0.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:24:57.001095Z",
     "start_time": "2025-06-28T10:24:48.737853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import faiss\n",
    "from scipy.stats import rankdata"
   ],
   "id": "48d30c2697654eb0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:24:57.038675Z",
     "start_time": "2025-06-28T10:24:57.016633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 1. LOAD DATA ===\n",
    "file_path = 'data/processed/cases.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path} not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Ensure case_id is unique\n",
    "if df[\"case_id\"].duplicated().any():\n",
    "    print(\"Error: Duplicate case_id found in dataset.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check for required columns\n",
    "required_columns = ['case_id', 'text_full', 'pidana_penjara']\n",
    "if not all(col in df for col in required_columns):\n",
    "    print(f\"Error: Dataset missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
    "    sys.exit(1)"
   ],
   "id": "402e57885ca3e7cc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:24:57.057976Z",
     "start_time": "2025-06-28T10:24:57.046213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 2. EXTRAK SOLUSI ===\n",
    "def normalize_solution(text: str) -> str:\n",
    "    \"\"\"Normalize solution text to standardize penalties (e.g., 'penjara 2 tahun').\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"Unknown\"\n",
    "    text = text.lower().strip()\n",
    "    # Extract penalty patterns (e.g., '2 tahun', '6 bulan', 'denda Rp 5 juta')\n",
    "    match = re.search(r'(penjara|denda)?\\s*(\\d+\\.?\\d*)\\s*(tahun|bulan|juta)?', text)\n",
    "    if match:\n",
    "        penalty_type = match.group(1) if match.group(1) else \"penjara\"  # Default to penjara if type is missing\n",
    "        value = match.group(2)\n",
    "        unit = match.group(3) if match.group(3) else \"\"\n",
    "        return f\"{penalty_type} {value} {unit}\".strip()\n",
    "    return text\n",
    "\n",
    "# Create case_solutions dictionary, skipping invalid entries\n",
    "case_solutions = {\n",
    "    row['case_id']: normalize_solution(str(row['pidana_penjara']))\n",
    "    for _, row in df.iterrows()\n",
    "    if pd.notna(row['pidana_penjara']) and str(row['pidana_penjara']).strip()\n",
    "}\n",
    "if len(case_solutions) < len(df):\n",
    "    print(f\"Warning: {len(df) - len(case_solutions)} cases with missing or empty pidana_penjara skipped.\")\n",
    "\n",
    "if not case_solutions:\n",
    "    print(\"Error: No valid solutions found in dataset.\")\n",
    "    sys.exit(1)"
   ],
   "id": "a891058468921541",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:41.844510Z",
     "start_time": "2025-06-28T10:24:57.084879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 3. PREPROCESSING ===\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stop_words_indonesia = stopword_factory.get_stop_words() + [\"terdakwa\", \"korban\", \"menyatakan\", \"secara\", \"sah\", \"meyakinkan\"]\n",
    "\n",
    "synonyms = {\n",
    "    \"pengeroyokan\": [\"kekerasan bersama-sama\", \"penganiayaan bersama-sama\"],\n",
    "    \"penganiayaan\": [\"kekerasan\", \"penyerangan\"],\n",
    "    \"turut serta\": [\"ikut serta\", \"bersama-sama\"],\n",
    "    \"luka berat\": [\"cedera parah\", \"luka serius\"]\n",
    "}\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"Preprocess text: lowercase, remove punctuation, stem, apply synonyms, remove stopwords.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = stemmer.stem(text)\n",
    "    for key, syn_list in synonyms.items():\n",
    "        for syn in syn_list:\n",
    "            text = text.replace(syn, key)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words_indonesia])\n",
    "    return text\n",
    "\n",
    "corpus = df[\"text_full\"].apply(preprocess)"
   ],
   "id": "f2df3ba837732ff5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.480532Z",
     "start_time": "2025-06-28T10:25:41.859350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 4. REPRESENTASI VEKTOR ===\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_indonesia, max_features=15000, sublinear_tf=True, ngram_range=(1, 3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# IndoBERT Embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = AutoModel.from_pretrained(\"./indobert_finetuned\" if os.path.exists(\"./indobert_finetuned\") else \"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "def bert_embed(text: str) -> np.ndarray:\n",
    "    \"\"\"Generate IndoBERT embeddings for text.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.max(dim=1).values.squeeze().numpy()\n",
    "        return embeddings / np.linalg.norm(embeddings) if np.linalg.norm(embeddings) != 0 else np.zeros(768)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {e}\")\n",
    "        return np.zeros(768)\n",
    "\n",
    "# Load or compute IndoBERT embeddings\n",
    "embeddings_file = 'data/processed/embeddings.json'\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    bert_embeddings = np.array([data[\"indobert_embedding\"] for data in embeddings_data])\n",
    "else:\n",
    "    bert_embeddings = np.array([bert_embed(text) for text in corpus])"
   ],
   "id": "1e879cc0e057d1c2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.533074Z",
     "start_time": "2025-06-28T10:25:45.520002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 5. SPLITTING DATA ===\n",
    "# Use pasal for stratification and as Naive Bayes target\n",
    "if \"pasal\" not in df:\n",
    "    print(\"Warning: 'pasal' column not found. Falling back to no stratification and skipping Naive Bayes training.\")\n",
    "    nb_trained = False\n",
    "    stratify_col = None\n",
    "    X_train_tfidf, X_test_tfidf, case_id_train, case_id_test = train_test_split(\n",
    "        tfidf_matrix, df[\"case_id\"], test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train_bert, X_test_bert = train_test_split(\n",
    "        bert_embeddings, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    nb_trained = True\n",
    "    stratify_col = df[\"pasal\"]\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "        tfidf_matrix, df[\"pasal\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )\n",
    "    X_train_bert, X_test_bert = train_test_split(\n",
    "        bert_embeddings, test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )\n",
    "    case_id_train, case_id_test = train_test_split(\n",
    "        df[\"case_id\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )"
   ],
   "id": "11f0a0a5d07d3f3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.576989Z",
     "start_time": "2025-06-28T10:25:45.564896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 6. MODEL RETRIEVAL: NAIVE BAYES ===\n",
    "if nb_trained:\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Evaluate Naive Bayes\n",
    "    y_pred = nb.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    prob_scores = nb.predict_proba(X_test_tfidf)\n",
    "    class_labels = np.unique(y_train)\n",
    "    log_losses = []\n",
    "\n",
    "    for i, y_true in enumerate(y_test):\n",
    "        if y_true not in class_labels:\n",
    "            print(f\"Warning: Test pasal {y_true} not in training set. Skipping.\")\n",
    "            continue\n",
    "        true_class_idx = np.where(class_labels == y_true)[0][0]\n",
    "        true_prob = prob_scores[i, true_class_idx]\n",
    "        loss = -np.log(max(true_prob, 1e-15))  # Avoid log(0)\n",
    "        log_losses.append(loss)\n",
    "\n",
    "    log_loss_value = np.mean(log_losses) if log_losses else 0.0\n",
    "    print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Naive Bayes Log Loss: {log_loss_value:.4f}\")\n",
    "else:\n",
    "    print(\"Naive Bayes training skipped due to missing 'pasal' column.\")"
   ],
   "id": "4fea52f1fad651b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 1.0000\n",
      "Naive Bayes Log Loss: 0.0339\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.614889Z",
     "start_time": "2025-06-28T10:25:45.608007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 7. FAISS INDEX FOR INDOBERT RETRIEVAL ===\n",
    "dimension = bert_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(bert_embeddings.astype(np.float32))"
   ],
   "id": "c01e8f4f22c9e310",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.663555Z",
     "start_time": "2025-06-28T10:25:45.651276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 8. FUNGSI RETRIEVAL ===\n",
    "def retrieve(query: str, k: int = 5, method: str = \"nb\") -> List[Tuple[int, float]]:\n",
    "    \"\"\"Retrieve top-k cases for a query using the specified method.\"\"\"\n",
    "    query_clean = preprocess(query)\n",
    "    try:\n",
    "        if method == \"bm25\":\n",
    "            query_tokens = query_clean.split()\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            top_k_idx = np.argsort(scores)[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"tfidf\":\n",
    "            query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "            scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            top_k_idx = scores.argsort()[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"bert\":\n",
    "            query_vec = bert_embed(query_clean).astype(np.float32)\n",
    "            query_vec = query_vec / np.linalg.norm(query_vec) if np.linalg.norm(query_vec) != 0 else np.zeros(dimension, dtype=np.float32)\n",
    "            scores, indices = index.search(query_vec.reshape(1, -1), k)\n",
    "            top_k_idx = indices[0]\n",
    "            scores = scores[0]\n",
    "        elif method == \"nb\":\n",
    "            if not nb_trained:\n",
    "                print(\"Warning: Naive Bayes not trained. Falling back to TF-IDF.\")\n",
    "                query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "                scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                top_k_idx = scores.argsort()[::-1][:k]\n",
    "                scores = scores[top_k_idx]\n",
    "            else:\n",
    "                query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "                predicted_pasal = nb.predict(query_vec)[0]\n",
    "                candidate_idx = df[df[\"pasal\"] == predicted_pasal].index\n",
    "                if len(candidate_idx) == 0:\n",
    "                    print(f\"Warning: No cases found for predicted pasal {predicted_pasal}. Falling back to TF-IDF.\")\n",
    "                    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                    top_k_idx = scores.argsort()[::-1][:k]\n",
    "                    scores = scores[top_k_idx]\n",
    "                else:\n",
    "                    candidate_matrix = tfidf_matrix[candidate_idx]\n",
    "                    scores = cosine_similarity(query_vec, candidate_matrix).flatten()\n",
    "                    top_k_idx = candidate_idx[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "                    scores = scores[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'bm25', 'tfidf', 'bert', or 'nb'.\")\n",
    "        top_case_ids = df.iloc[top_k_idx][\"case_id\"].tolist()\n",
    "        return list(zip(top_case_ids, scores))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval for query '{query}' with method '{method}': {e}\")\n",
    "        return []"
   ],
   "id": "7d5fa80fd41f3042",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.719814Z",
     "start_time": "2025-06-28T10:25:45.712202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 9. PREDIKSI OUTCOME ===\n",
    "def predict_outcome(query: str, k: int = 5, method: str = \"nb\", strategy: str = \"weighted\") -> str:\n",
    "    \"\"\"\n",
    "    Predict the outcome for a query based on top-k similar cases.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query describing the case.\n",
    "        k (int): Number of top cases to retrieve (default: 5).\n",
    "        method (str): Retrieval method ('bm25', 'tfidf', 'bert', 'nb').\n",
    "        strategy (str): Prediction strategy ('majority' or 'weighted').\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted solution text.\n",
    "    \"\"\"\n",
    "    top_k = retrieve(query, k=k, method=method)\n",
    "    if not top_k:\n",
    "        return \"No similar cases found.\"\n",
    "\n",
    "    case_ids, scores = zip(*top_k)\n",
    "    solutions = [case_solutions[cid] for cid in case_ids if cid in case_solutions]\n",
    "\n",
    "    if not solutions:\n",
    "        return \"No solutions found for retrieved cases.\"\n",
    "\n",
    "    if strategy == \"majority\":\n",
    "        # Majority voting: select the most frequent solution\n",
    "        solution_counts = Counter(solutions)\n",
    "        predicted_solution = solution_counts.most_common(1)[0][0]\n",
    "    else:  # Weighted similarity\n",
    "        # Sum similarity scores for each unique solution\n",
    "        solution_scores = {}\n",
    "        for sol, score in zip(solutions, scores):\n",
    "            solution_scores[sol] = solution_scores.get(sol, 0.0) + score\n",
    "        predicted_solution = max(solution_scores, key=solution_scores.get)\n",
    "\n",
    "    return predicted_solution"
   ],
   "id": "d5945241d12a1258",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.901083Z",
     "start_time": "2025-06-28T10:25:45.770416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 10. DEMO MANUAL ===\n",
    "# Configuration\n",
    "RETRIEVAL_METHOD = \"nb\"  # Options: 'nb', 'bm25', 'tfidf', 'bert'\n",
    "PREDICTION_STRATEGY = \"weighted\"  # Options: 'majority', 'weighted'\n",
    "\n",
    "# Define 5 new case queries (some with ground-truth for demo)\n",
    "new_queries = [\n",
    "    {\"query_id\": 1, \"query\": \"Terdakwa melakukan penganiayaan ringan di tempat umum\", \"ground_truth\": \"penjara 6 bulan\"},\n",
    "    {\"query_id\": 2, \"query\": \"Terdakwa bersama-sama melakukan pengeroyokan menyebabkan luka berat\", \"ground_truth\": \"penjara 3 tahun\"},\n",
    "    {\"query_id\": 3, \"query\": \"Terdakwa melakukan penggelapan dalam jabatan\", \"ground_truth\": None},\n",
    "    {\"query_id\": 4, \"query\": \"Terdakwa menghasut orang untuk melakukan kekerasan\", \"ground_truth\": None},\n",
    "    {\"query_id\": 5, \"query\": \"Terdakwa melakukan kekerasan terhadap anak di bawah umur\", \"ground_truth\": \"\"}\n",
    "]\n",
    "\n",
    "# Run predict_outcome and store results\n",
    "results = []\n",
    "exact_matches = 0\n",
    "total_with_ground_truth = 0\n",
    "\n",
    "for q in new_queries:\n",
    "    query_id = q[\"query_id\"]\n",
    "    query = q[\"query\"]\n",
    "    ground_truth = q[\"ground_truth\"]\n",
    "\n",
    "    # Predict outcome\n",
    "    predicted_solution = predict_outcome(query, k=5, method=RETRIEVAL_METHOD, strategy=PREDICTION_STRATEGY)\n",
    "    top_k = retrieve(query, k=5, method=RETRIEVAL_METHOD)\n",
    "    top_case_ids = [cid for cid, _ in top_k]\n",
    "\n",
    "    # Evaluate if ground truth exists\n",
    "    is_correct = False\n",
    "    if ground_truth is not None:\n",
    "        total_with_ground_truth += 1\n",
    "        normalized_ground_truth = normalize_solution(ground_truth)\n",
    "        is_correct = normalized_ground_truth == predicted_solution\n",
    "        if is_correct:\n",
    "            exact_matches += 1\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Query ID: {query_id}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Predicted Solution: {predicted_solution}\")\n",
    "    print(f\"Top-5 Case IDs: {top_case_ids}\")\n",
    "    print(f\"Ground Truth: {'Not available' if ground_truth is None else ground_truth}\")\n",
    "    if ground_truth is not None:\n",
    "        print(f\"Match: {'Yes' if is_correct else 'No'}\")\n",
    "    print(\"---\")\n",
    "\n",
    "    results.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"predicted_solution\": predicted_solution,\n",
    "        \"top_5_case_ids\": \",\".join(map(str, top_case_ids))\n",
    "    })\n",
    "\n",
    "# Print evaluation metrics\n",
    "if total_with_ground_truth > 0:\n",
    "    accuracy = exact_matches / total_with_ground_truth\n",
    "    print(f\"Exact Match Accuracy: {accuracy:.4f} ({exact_matches}/{total_with_ground_truth})\")\n",
    "else:\n",
    "    print(\"No ground-truth solutions available for accuracy calculation.\")"
   ],
   "id": "543b80f8d01f8b35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1\n",
      "Query: Terdakwa melakukan penganiayaan ringan di tempat umum\n",
      "Predicted Solution: penjara 3\n",
      "Top-5 Case IDs: [64, 5, 66, 70, 17]\n",
      "Ground Truth: penjara 6 bulan\n",
      "Match: No\n",
      "---\n",
      "Query ID: 2\n",
      "Query: Terdakwa bersama-sama melakukan pengeroyokan menyebabkan luka berat\n",
      "Predicted Solution: penjara 3\n",
      "Top-5 Case IDs: [82, 75, 28, 76, 14]\n",
      "Ground Truth: penjara 3 tahun\n",
      "Match: No\n",
      "---\n",
      "Query ID: 3\n",
      "Query: Terdakwa melakukan penggelapan dalam jabatan\n",
      "Predicted Solution: penjara 1\n",
      "Top-5 Case IDs: [60, 53, 18, 63, 39]\n",
      "Ground Truth: Not available\n",
      "---\n",
      "Query ID: 4\n",
      "Query: Terdakwa menghasut orang untuk melakukan kekerasan\n",
      "Predicted Solution: penjara 2\n",
      "Top-5 Case IDs: [6, 30, 8, 76, 80]\n",
      "Ground Truth: Not available\n",
      "---\n",
      "Query ID: 5\n",
      "Query: Terdakwa melakukan kekerasan terhadap anak di bawah umur\n",
      "Predicted Solution: penjara 2\n",
      "Top-5 Case IDs: [32, 6, 80, 50, 30]\n",
      "Ground Truth: \n",
      "Match: No\n",
      "---\n",
      "Exact Match Accuracy: 0.0000 (0/3)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:25:45.999222Z",
     "start_time": "2025-06-28T10:25:45.986576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 11. SAVE OUTPUT ===\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"data/results/predictions.csv\", index=False)\n",
    "print(f\"Results saved to data/results/predictions.csv\")"
   ],
   "id": "966790717f365fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data/results/predictions.csv\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
