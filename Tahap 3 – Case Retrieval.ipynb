{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:21:56.367623Z",
     "start_time": "2025-06-28T12:21:54.226898Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers torch sentence-transformers Sastrawi scikit-learn faiss-cpu rank_bm25",
   "id": "cc7c01a7d5e290be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (1.11.0)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (0.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yurdan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:21:56.419607Z",
     "start_time": "2025-06-28T12:21:56.411607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import faiss\n",
    "from typing import List, Tuple"
   ],
   "id": "b305fb661025d2f9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:21:56.481602Z",
     "start_time": "2025-06-28T12:21:56.450189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 1. LOAD DATA ===\n",
    "file_path = 'data/processed/cases.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path} not found.\")\n",
    "    exit(1)"
   ],
   "id": "36256e596074fde8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:34.375931Z",
     "start_time": "2025-06-28T12:21:56.503099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 2. PREPROCESSING ===\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stop_words_indonesia = stopword_factory.get_stop_words() + [\"terdakwa\", \"korban\", \"menyatakan\", \"secara\", \"sah\", \"meyakinkan\"]\n",
    "\n",
    "synonyms = {\n",
    "    \"pengeroyokan\": [\"kekerasan bersama-sama\", \"penganiayaan bersama-sama\"],\n",
    "    \"penganiayaan\": [\"kekerasan\", \"penyerangan\"],\n",
    "    \"turut serta\": [\"ikut serta\", \"bersama-sama\"],\n",
    "    \"luka berat\": [\"cedera parah\", \"luka serius\"]\n",
    "}\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = stemmer.stem(text)\n",
    "    for key, syn_list in synonyms.items():\n",
    "        for syn in syn_list:\n",
    "            text = text.replace(syn, key)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words_indonesia])\n",
    "    return text\n",
    "\n",
    "corpus = df[\"text_full\"].apply(preprocess)"
   ],
   "id": "eb7cae3216d4f566",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.623794Z",
     "start_time": "2025-06-28T12:22:34.384150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 3. REPRESENTASI VEKTOR ===\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_indonesia, max_features=15000, sublinear_tf=True, ngram_range=(1, 3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# IndoBERT Embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = AutoModel.from_pretrained(\"./indobert_finetuned\" if os.path.exists(\"./indobert_finetuned\") else \"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "def bert_embed(text: str) -> np.ndarray:\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.max(dim=1).values.squeeze().numpy()\n",
    "        return embeddings / np.linalg.norm(embeddings) if np.linalg.norm(embeddings) != 0 else np.zeros(768)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {e}\")\n",
    "        return np.zeros(768)\n",
    "\n",
    "# Load or compute IndoBERT embeddings\n",
    "embeddings_file = 'data/processed/embeddings.json'\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    bert_embeddings = np.array([data[\"indobert_embedding\"] for data in embeddings_data])\n",
    "else:\n",
    "    bert_embeddings = np.array([bert_embed(text) for text in corpus])"
   ],
   "id": "2e156bc79614ae6f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.678803Z",
     "start_time": "2025-06-28T12:22:37.667330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 4. SPLITTING DATA ===\n",
    "if \"pasal\" not in df:\n",
    "    print(\"Warning: 'pasal' column not found. Falling back to no stratification and skipping Naive Bayes training.\")\n",
    "    nb_trained = False\n",
    "    stratify_col = None\n",
    "    X_train_tfidf, X_test_tfidf, case_id_train, case_id_test = train_test_split(\n",
    "        tfidf_matrix, df[\"case_id\"], test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train_bert, X_test_bert = train_test_split(\n",
    "        bert_embeddings, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    nb_trained = True\n",
    "    stratify_col = df[\"pasal\"]\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "        tfidf_matrix, df[\"pasal\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )\n",
    "    X_train_bert, X_test_bert = train_test_split(\n",
    "        bert_embeddings, test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )\n",
    "    case_id_train, case_id_test = train_test_split(\n",
    "        df[\"case_id\"], test_size=0.2, random_state=42, stratify=stratify_col\n",
    "    )"
   ],
   "id": "80d30f02f68bd006",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.708179Z",
     "start_time": "2025-06-28T12:22:37.702364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 5. MODEL RETRIEVAL: NAIVE BAYES ===\n",
    "if nb_trained:\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_tfidf, y_train)"
   ],
   "id": "b666f2cf98c8ebc4",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.739333Z",
     "start_time": "2025-06-28T12:22:37.735589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 6. FAISS INDEX FOR INDOBERT RETRIEVAL ===\n",
    "dimension = bert_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(bert_embeddings.astype(np.float32))"
   ],
   "id": "875381b13ab9307d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.778846Z",
     "start_time": "2025-06-28T12:22:37.768007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 7. FUNGSI RETRIEVAL ===\n",
    "def retrieve(query: str, k: int = 5, method: str = \"bm25\") -> List[Tuple[int, float]]:\n",
    "    query_clean = preprocess(query)\n",
    "    try:\n",
    "        if method == \"bm25\":\n",
    "            query_tokens = query_clean.split()\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            top_k_idx = np.argsort(scores)[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"tfidf\":\n",
    "            query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "            scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            top_k_idx = scores.argsort()[::-1][:k]\n",
    "            scores = scores[top_k_idx]\n",
    "        elif method == \"bert\":\n",
    "            query_vec = bert_embed(query_clean).astype(np.float32)\n",
    "            query_vec = query_vec / np.linalg.norm(query_vec) if np.linalg.norm(query_vec) != 0 else np.zeros(dimension, dtype=np.float32)\n",
    "            scores, indices = index.search(query_vec.reshape(1, -1), k)\n",
    "            top_k_idx = indices[0]\n",
    "            scores = scores[0]\n",
    "        elif method == \"nb\":\n",
    "            if not nb_trained:\n",
    "                print(\"Warning: Naive Bayes not trained. Falling back to TF-IDF.\")\n",
    "                query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "                scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                top_k_idx = scores.argsort()[::-1][:k]\n",
    "                scores = scores[top_k_idx]\n",
    "            else:\n",
    "                query_vec = tfidf_vectorizer.transform([query_clean])\n",
    "                predicted_pasal = nb.predict(query_vec)[0]\n",
    "                candidate_idx = df[df[\"pasal\"] == predicted_pasal].index\n",
    "                if len(candidate_idx) == 0:\n",
    "                    print(f\"Warning: No cases found for predicted pasal {predicted_pasal}. Falling back to TF-IDF.\")\n",
    "                    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "                    top_k_idx = scores.argsort()[::-1][:k]\n",
    "                    scores = scores[top_k_idx]\n",
    "                else:\n",
    "                    candidate_matrix = tfidf_matrix[candidate_idx]\n",
    "                    scores = cosine_similarity(query_vec, candidate_matrix).flatten()\n",
    "                    top_k_idx = candidate_idx[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "                    scores = scores[scores.argsort()[::-1][:min(k, len(scores))]]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'bm25', 'tfidf', 'bert', or 'nb'.\")\n",
    "        top_case_ids = df.iloc[top_k_idx][\"case_id\"].tolist()\n",
    "        return list(zip(top_case_ids, scores))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval for query '{query}' with method '{method}': {e}\")\n",
    "        return []"
   ],
   "id": "fd3b111ef561ca97",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.808199Z",
     "start_time": "2025-06-28T12:22:37.802172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 8. PENGUJIAN AWAL ===\n",
    "query_eval = [\n",
    "    {\"query\": \"Terdakwa melakukan penganiayaan yang menyebabkan luka\", \"ground_truth\": 7},\n",
    "    {\"query\": \"Terdakwa bersama-sama melakukan kekerasan terhadap orang di muka umum\", \"ground_truth\": 6},\n",
    "    {\"query\": \"Terdakwa melakukan penganiayaan berat yang mengakibatkan luka berat\", \"ground_truth\": 14},\n",
    "    {\"query\": \"Terdakwa turut serta dalam penganiayaan secara bersama-sama\", \"ground_truth\": 5},\n",
    "    {\"query\": \"Terdakwa melakukan kekerasan terhadap pegawai negeri yang menyebabkan luka\", \"ground_truth\": 1},\n",
    "    {\"query\": \"Terdakwa melakukan penganiayaan yang mengakibatkan kematian\", \"ground_truth\": 20},\n",
    "    {\"query\": \"Terdakwa dengan terang-terangan menggunakan kekerasan terhadap orang\", \"ground_truth\": 12},\n",
    "    {\"query\": \"Terdakwa melakukan penggelapan barang karena hubungan kerja\", \"ground_truth\": 71},\n",
    "    {\"query\": \"Terdakwa menghasut untuk melakukan perbuatan pidana di muka umum\", \"ground_truth\": 85},\n",
    "    {\"query\": \"Terdakwa bersama-sama melakukan pengeroyokan yang menyebabkan luka\", \"ground_truth\": 90}\n",
    "]\n",
    "\n",
    "# Filter valid queries\n",
    "valid_query_eval = [q for q in query_eval if q[\"ground_truth\"] in df[\"case_id\"].values]\n",
    "if len(valid_query_eval) < len(query_eval):\n",
    "    print(f\"Warning: {len(query_eval) - len(valid_query_eval)} queries skipped due to invalid ground_truth case_id.\")\n",
    "if not valid_query_eval:\n",
    "    print(\"Error: No valid queries available for evaluation. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "with open(\"data/eval/queries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(valid_query_eval, f, indent=2, ensure_ascii=False)"
   ],
   "id": "afc53b60b9804922",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:37.838795Z",
     "start_time": "2025-06-28T12:22:37.833310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 9. EVALUATE RETRIEVAL ===\n",
    "def evaluate_retrieval(query_eval, k=5, method=\"bm25\"):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    failure_cases = []\n",
    "\n",
    "    for q in query_eval:\n",
    "        results = retrieve(q[\"query\"], k, method)\n",
    "        if not results:\n",
    "            y_true.append(1)  # Assume relevant\n",
    "            y_pred.append(0)  # No cases retrieved\n",
    "            failure_cases.append({\n",
    "                \"query\": q[\"query\"],\n",
    "                \"ground_truth\": q[\"ground_truth\"],\n",
    "                \"top_k_case_ids\": [],\n",
    "                \"reason\": \"No cases retrieved\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        case_ids, _ = zip(*results)\n",
    "        gt = q[\"ground_truth\"]\n",
    "        # Consider retrieval correct if ground-truth case_id is in top-k\n",
    "        is_relevant = 1 if gt in case_ids else 0\n",
    "        y_true.append(1)  # Query has a relevant case\n",
    "        y_pred.append(is_relevant)\n",
    "\n",
    "        if not is_relevant:\n",
    "            failure_cases.append({\n",
    "                \"query\": q[\"query\"],\n",
    "                \"ground_truth\": gt,\n",
    "                \"top_k_case_ids\": list(case_ids),\n",
    "                \"reason\": \"Ground-truth case not in top-k\"\n",
    "            })\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred) if y_true else 0.0,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Save failure cases\n",
    "    os.makedirs(\"data/eval\", exist_ok=True)\n",
    "    with open(f\"data/eval/failure_cases_retrieval_{method}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(failure_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return metrics"
   ],
   "id": "d137a3495d377e99",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:22:38.177491Z",
     "start_time": "2025-06-28T12:22:37.864858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 10. EVALUATE ALL METHODS AND DISPLAY METRICS ===\n",
    "if valid_query_eval:\n",
    "    methods = [\"bm25\", \"tfidf\", \"bert\", \"nb\"]\n",
    "    metrics_list = []\n",
    "\n",
    "    for method in methods:\n",
    "        metrics = evaluate_retrieval(valid_query_eval, k=5, method=method)\n",
    "        metrics_list.append({\n",
    "            \"Model\": method.upper(),\n",
    "            \"Accuracy\": metrics[\"accuracy\"],\n",
    "            \"Precision\": metrics[\"precision\"],\n",
    "            \"Recall\": metrics[\"recall\"],\n",
    "            \"F1-Score\": metrics[\"f1_score\"]\n",
    "        })\n",
    "\n",
    "    # Create metrics table\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "    # Display metrics table\n",
    "    print(\"\\n=== Retrieval Performance Metrics ===\")\n",
    "    print(metrics_df.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    os.makedirs(\"data/eval\", exist_ok=True)\n",
    "    metrics_df.to_csv(\"data/eval/retrieval_metrics.csv\", index=False)\n",
    "    print(\"\\nMetrics saved to data/eval/retrieval_metrics.csv\")\n",
    "else:\n",
    "    print(\"No evaluation performed due to empty valid_query_eval.\")"
   ],
   "id": "b190103aa665528b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieval Performance Metrics ===\n",
      "Model  Accuracy  Precision  Recall  F1-Score\n",
      " BM25    0.6000     1.0000  0.6000    0.7500\n",
      "TFIDF    0.7000     1.0000  0.7000    0.8235\n",
      " BERT    0.1000     1.0000  0.1000    0.1818\n",
      "   NB    0.6000     1.0000  0.6000    0.7500\n",
      "\n",
      "Metrics saved to data/eval/retrieval_metrics.csv\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
